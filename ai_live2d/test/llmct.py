#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
LLMå®¢æˆ·ç«¯æµ‹è¯•è„šæœ¬ - å®Œå…¨å¼‚æ­¥ç‰ˆæœ¬
"""

import sys
import os
import asyncio
import logging
import json
import aiohttp
from unittest.mock import AsyncMock

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.DEBUG, 
                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger("test_llm_client")

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# å¯¼å…¥LLMå®¢æˆ·ç«¯å’Œäº‹ä»¶æ€»çº¿
from core.event_bus import EventBus

# å°è¯•å¤šä¸ªå¯èƒ½çš„é…ç½®è·¯å¾„
possible_config_paths = [
    os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "config.json"),  # é¡¹ç›®æ ¹ç›®å½•
    "config.json",  # å½“å‰ç›®å½•
    os.path.join(os.path.dirname(os.path.abspath(__file__)), "config.json"),  # æµ‹è¯•è„šæœ¬ç›®å½•
]

config = None
for config_path in possible_config_paths:
    try:
        if os.path.exists(config_path):
            logger.info(f"å°è¯•åŠ è½½é…ç½®: {config_path}")
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            logger.info(f"æˆåŠŸåŠ è½½é…ç½®: {config_path}")
            break
    except Exception as e:
        logger.warning(f"åŠ è½½é…ç½®å¤±è´¥: {config_path} - {e}")

# å¦‚æœæ²¡æœ‰æ‰¾åˆ°é…ç½®æ–‡ä»¶ï¼Œä½¿ç”¨æ¨¡æ‹ŸAPI
if not config:
    logger.warning("æœªæ‰¾åˆ°é…ç½®æ–‡ä»¶ï¼Œä½¿ç”¨æ¨¡æ‹ŸAPIæµ‹è¯•")
    # åˆ›å»ºæµ‹è¯•é…ç½®
    config = {
        "llm": {
            "api_key": "test_key",
            "api_url": "http://localhost:8000/v1",
            "model": "test-model",
            "system_prompt": "ä½ æ˜¯ä¸€ä¸ªæµ‹è¯•åŠ©æ‰‹ã€‚"
        },
        "context": {
            "enable_limit": True,
            "max_messages": 10
        }
    }
    use_mock = True
else:
    use_mock = False

# æ¨¡æ‹ŸLLMå®¢æˆ·ç«¯ç±» - å¼‚æ­¥ç‰ˆæœ¬
class MockLLMClient:
    def __init__(self, config=None, event_bus=None):
        self.config = config or {}
        self.event_bus = event_bus
        self.messages = []
        
        # æ·»åŠ ç³»ç»Ÿæç¤ºè¯
        system_prompt = self.config.get("llm", {}).get("system_prompt")
        if system_prompt:
            self.messages.append({
                "role": "system",
                "content": system_prompt
            })
        
        logger.info("æ¨¡æ‹ŸLLMå®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ")
    
    async def set_system_prompt(self, prompt):
        """è®¾ç½®ç³»ç»Ÿæç¤ºè¯ - å¼‚æ­¥ç‰ˆæœ¬"""
        # ç§»é™¤æ—§çš„ç³»ç»Ÿæç¤ºè¯
        self.messages = [msg for msg in self.messages if msg["role"] != "system"]
        
        # æ·»åŠ æ–°çš„ç³»ç»Ÿæç¤ºè¯
        if prompt:
            self.messages.insert(0, {
                "role": "system",
                "content": prompt
            })
            
        logger.info("æ¨¡æ‹Ÿç³»ç»Ÿæç¤ºè¯å·²æ›´æ–°")
        
        # å‘å¸ƒäº‹ä»¶
        if self.event_bus:
            await self.event_bus.publish("llm_system_prompt_updated", {
                "prompt": prompt
            })
    
    async def send_message(self, text, image_path=None, stream=True):
        """å‘é€æ¶ˆæ¯ - å¼‚æ­¥ç‰ˆæœ¬"""
        # å‘å¸ƒå¼€å§‹äº‹ä»¶
        if self.event_bus:
            await self.event_bus.publish("llm_start", {
                "text": text,
                "has_image": image_path is not None
            })
        
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
        self.messages.append({
            "role": "user",
            "content": text
        })
        
        # æ¨¡æ‹Ÿå¤„ç†å»¶è¿Ÿ
        await asyncio.sleep(0.5)
        
        # ç”Ÿæˆæ¨¡æ‹Ÿå“åº”
        if "é”™è¯¯" in text or "error" in text.lower():
            # æ¨¡æ‹Ÿé”™è¯¯å“åº”
            if self.event_bus:
                await self.event_bus.publish("llm_error", {
                    "error": "æ¨¡æ‹Ÿé”™è¯¯ï¼šè¯·æ±‚åŒ…å«é”™è¯¯å…³é”®è¯"
                })
            raise Exception("æ¨¡æ‹Ÿé”™è¯¯ï¼šè¯·æ±‚åŒ…å«é”™è¯¯å…³é”®è¯")
        
        # ç”Ÿæˆä¸åŒç±»å‹çš„å“åº”
        if "é•¿å›å¤" in text:
            response = "è¿™æ˜¯ä¸€ä¸ªå¾ˆé•¿å¾ˆé•¿çš„å›å¤ï¼Œç”¨æ¥æµ‹è¯•æµå¼å“åº”åŠŸèƒ½ã€‚å®ƒåŒ…å«å¾ˆå¤šå­—ç¬¦ï¼Œå¯ä»¥å¾ˆå¥½åœ°å±•ç¤ºé€å­—è¾“å‡ºçš„æ•ˆæœã€‚"
        elif "ç®€çŸ­" in text:
            response = "å¥½çš„ï¼"
        else:
            response = f"è¿™æ˜¯å¯¹'{text}'çš„æ¨¡æ‹Ÿå›å¤ï¼ŒåŒ…å«{len(text)}ä¸ªå­—ç¬¦ã€‚"
        
        # æ¨¡æ‹Ÿæµå¼å“åº”
        if stream and self.event_bus:
            for i, char in enumerate(response):
                await self.event_bus.publish("llm_streaming", {
                    "text": char,
                    "full_text": response[:i+1],
                    "is_final": False
                })
                await asyncio.sleep(0.02)  # æ¨¡æ‹Ÿæµå¼å“åº”å»¶è¿Ÿ
            
            # å‘å¸ƒæµå¼å“åº”ç»“æŸ
            await self.event_bus.publish("llm_streaming", {
                "text": "",
                "full_text": response,
                "is_final": True
            })
        
        # æ·»åŠ åŠ©æ‰‹å›å¤
        self.messages.append({
            "role": "assistant",
            "content": response
        })
        
        # å‘å¸ƒå®Œæˆäº‹ä»¶
        if self.event_bus:
            await self.event_bus.publish("llm_complete", {
                "text": response,
                "message_count": len(self.messages)
            })
        
        return response
    
    def get_messages(self):
        """è·å–æ¶ˆæ¯å†å²"""
        return self.messages.copy()
    
    async def get_context_info(self):
        """è·å–ä¸Šä¸‹æ–‡ä¿¡æ¯"""
        system_count = len([msg for msg in self.messages if msg["role"] == "system"])
        user_count = len([msg for msg in self.messages if msg["role"] == "user"])
        assistant_count = len([msg for msg in self.messages if msg["role"] == "assistant"])
        
        return {
            "total_messages": len(self.messages),
            "system_messages": system_count,
            "user_messages": user_count,
            "assistant_messages": assistant_count,
            "max_messages": 10,
            "enable_limit": True
        }
    
    async def clear_messages(self, keep_system=True):
        """æ¸…ç©ºæ¶ˆæ¯å†å²"""
        if keep_system:
            self.messages = [msg for msg in self.messages if msg["role"] == "system"]
        else:
            self.messages = []
        
        if self.event_bus:
            await self.event_bus.publish("llm_messages_cleared", {
                "keep_system": keep_system,
                "remaining_count": len(self.messages)
            })

async def test_api_connectivity():
    """æµ‹è¯•APIè¿æ¥æ€§"""
    if use_mock:
        logger.info("ä½¿ç”¨æ¨¡æ‹ŸAPIï¼Œè·³è¿‡è¿æ¥æ€§æµ‹è¯•")
        return True
    
    try:
        api_url = config.get("llm", {}).get("api_url", "")
        if not api_url:
            logger.warning("é…ç½®ä¸­æœªæ‰¾åˆ°API URL")
            return False
        
        # ç®€å•çš„è¿æ¥æµ‹è¯•
        async with aiohttp.ClientSession() as session:
            try:
                async with session.get(api_url, timeout=5) as response:
                    logger.info(f"APIè¿æ¥æµ‹è¯•: {response.status}")
                    return response.status in [200, 404, 401]  # è¿™äº›çŠ¶æ€ç è¯´æ˜æœåŠ¡åœ¨è¿è¡Œ
            except Exception as e:
                logger.warning(f"APIè¿æ¥æµ‹è¯•å¤±è´¥: {e}")
                return False
    except Exception as e:
        logger.error(f"APIè¿æ¥æ€§æµ‹è¯•é”™è¯¯: {e}")
        return False

async def test_llm_client():
    """æµ‹è¯•LLMå®¢æˆ·ç«¯çš„åŸºæœ¬åŠŸèƒ½"""
    global config, use_mock
    
    event_bus = EventBus()
    
    # è®°å½•äº‹ä»¶
    streaming_chunks = []
    complete_responses = []
    errors = []
    
    # äº‹ä»¶å›è°ƒ - å¼‚æ­¥ç‰ˆæœ¬
    async def on_llm_start(data):
        text = data.get("text", "")
        has_image = data.get("has_image", False)
        logger.info(f"ğŸš€ LLMå¼€å§‹å¤„ç†: {text} (åŒ…å«å›¾ç‰‡: {has_image})")
    
    async def on_llm_streaming(data):
        text = data.get("text", "")
        full_text = data.get("full_text", "")
        is_final = data.get("is_final", False)
        
        if text:
            print(text, end="", flush=True)  # å®æ—¶æ˜¾ç¤ºæµå¼è¾“å‡º
            streaming_chunks.append(text)
        
        if is_final:
            print()  # æ¢è¡Œ
            logger.info("ğŸ“ æµå¼å“åº”ç»“æŸ")
    
    async def on_llm_complete(data):
        text = data.get("text", "")
        message_count = data.get("message_count", 0)
        complete_responses.append(text)
        logger.info(f"âœ… LLMå®Œæˆå“åº”: {len(text)}å­—ç¬¦, æ¶ˆæ¯æ•°: {message_count}")
    
    async def on_llm_error(data):
        error = data.get("error", "")
        errors.append(error)
        logger.error(f"âŒ LLMé”™è¯¯: {error}")
    
    async def on_system_prompt_updated(data):
        prompt = data.get("prompt", "")
        logger.info(f"ğŸ“‹ ç³»ç»Ÿæç¤ºè¯å·²æ›´æ–°: {prompt[:50]}...")
    
    llm_client = None
    try:
        # è®¢é˜…äº‹ä»¶ï¼ˆæ³¨æ„éœ€è¦awaitï¼‰
        await event_bus.subscribe("llm_start", on_llm_start)
        await event_bus.subscribe("llm_streaming", on_llm_streaming)
        await event_bus.subscribe("llm_complete", on_llm_complete)
        await event_bus.subscribe("llm_error", on_llm_error)
        await event_bus.subscribe("llm_system_prompt_updated", on_system_prompt_updated)
        
        print("\n" + "="*60)
        print("ğŸ§ª LLMå®¢æˆ·ç«¯æµ‹è¯•")
        print("="*60)
        
        # æµ‹è¯•APIè¿æ¥æ€§
        if not use_mock:
            logger.info("ğŸ” æµ‹è¯•APIè¿æ¥æ€§...")
            if not await test_api_connectivity():
                logger.warning("APIè¿æ¥å¤±è´¥ï¼Œåˆ‡æ¢åˆ°æ¨¡æ‹Ÿæ¨¡å¼")
                use_mock = True
        
        # åˆ›å»ºLLMå®¢æˆ·ç«¯
        if use_mock:
            logger.info("ğŸ­ ä½¿ç”¨æ¨¡æ‹ŸLLMå®¢æˆ·ç«¯")
            llm_client = MockLLMClient(config, event_bus)
        else:
            logger.info("ğŸŒ ä½¿ç”¨çœŸå®LLMå®¢æˆ·ç«¯")
            from ai.llm_client import LLMClient
            llm_client = LLMClient(config, event_bus)
        
        # æµ‹è¯•1: ç³»ç»Ÿæç¤ºè¯è®¾ç½®
        logger.info("\nğŸ§ª æµ‹è¯•1: ç³»ç»Ÿæç¤ºè¯è®¾ç½®")
        system_prompt = "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„AIåŠ©æ‰‹ï¼Œç”¨ç®€æ´çš„è¯­è¨€å›ç­”é—®é¢˜ã€‚"
        await llm_client.set_system_prompt(system_prompt)
        
        # æµ‹è¯•2: åŸºæœ¬å¯¹è¯
        logger.info("\nğŸ§ª æµ‹è¯•2: åŸºæœ¬å¯¹è¯")
        test_message = "ä½ å¥½ï¼Œè¯·ç®€å•ä»‹ç»ä¸€ä¸‹è‡ªå·±ã€‚"
        logger.info(f"ğŸ“¤ å‘é€æ¶ˆæ¯: {test_message}")
        print("ğŸ¤– AIå›å¤: ", end="")
        
        try:
            response = await asyncio.wait_for(
                llm_client.send_message(test_message, stream=True), 
                timeout=10.0
            )
            logger.info(f"âœ… æ”¶åˆ°å®Œæ•´å“åº”: {len(response)}å­—ç¬¦")
        except asyncio.TimeoutError:
            logger.error("â° LLMå“åº”è¶…æ—¶")
            if not use_mock:
                raise
        
        # æµ‹è¯•3: çŸ­å›å¤
        logger.info("\nğŸ§ª æµ‹è¯•3: çŸ­å›å¤")
        short_message = "è¯·ç»™æˆ‘ä¸€ä¸ªç®€çŸ­çš„å›å¤"
        logger.info(f"ğŸ“¤ å‘é€æ¶ˆæ¯: {short_message}")
        print("ğŸ¤– AIå›å¤: ", end="")
        
        response2 = await llm_client.send_message(short_message)
        
        # æµ‹è¯•4: é•¿å›å¤ï¼ˆæµå¼æµ‹è¯•ï¼‰
        logger.info("\nğŸ§ª æµ‹è¯•4: é•¿å›å¤ï¼ˆæµå¼æµ‹è¯•ï¼‰")
        long_message = "è¯·ç»™æˆ‘ä¸€ä¸ªé•¿å›å¤æ¥æµ‹è¯•æµå¼è¾“å‡º"
        logger.info(f"ğŸ“¤ å‘é€æ¶ˆæ¯: {long_message}")
        print("ğŸ¤– AIå›å¤: ", end="")
        
        response3 = await llm_client.send_message(long_message, stream=True)
        
        # æµ‹è¯•5: ä¸Šä¸‹æ–‡ä¿¡æ¯
        logger.info("\nğŸ§ª æµ‹è¯•5: ä¸Šä¸‹æ–‡ä¿¡æ¯")
        context_info = await llm_client.get_context_info()
        logger.info(f"ğŸ“Š ä¸Šä¸‹æ–‡ä¿¡æ¯: {context_info}")
        
        # æµ‹è¯•6: æ¶ˆæ¯å†å²
        logger.info("\nğŸ§ª æµ‹è¯•6: æ¶ˆæ¯å†å²")
        messages = llm_client.get_messages()
        logger.info(f"ğŸ“š æ¶ˆæ¯å†å²æ•°é‡: {len(messages)}")
        for i, msg in enumerate(messages):
            role = msg["role"]
            content = str(msg["content"])[:50] + "..." if len(str(msg["content"])) > 50 else str(msg["content"])
            logger.info(f"  {i+1}. {role}: {content}")
        
        # æµ‹è¯•7: é”™è¯¯å¤„ç†ï¼ˆä»…æ¨¡æ‹Ÿæ¨¡å¼ï¼‰
        if use_mock:
            logger.info("\nğŸ§ª æµ‹è¯•7: é”™è¯¯å¤„ç†")
            try:
                await llm_client.send_message("è¯·è§¦å‘ä¸€ä¸ªé”™è¯¯")
            except Exception as e:
                logger.info(f"âœ… æˆåŠŸæ•è·é¢„æœŸé”™è¯¯: {e}")
        
        # æµ‹è¯•8: æ¸…ç©ºæ¶ˆæ¯
        logger.info("\nğŸ§ª æµ‹è¯•8: æ¸…ç©ºæ¶ˆæ¯")
        await llm_client.clear_messages(keep_system=True)
        final_messages = llm_client.get_messages()
        logger.info(f"ğŸ§¹ æ¸…ç©ºåæ¶ˆæ¯æ•°é‡: {len(final_messages)}")
        
        # æµ‹è¯•ç»“æœç»Ÿè®¡
        print("\n" + "="*60)
        print("ğŸ“Š æµ‹è¯•ç»“æœç»Ÿè®¡")
        print("="*60)
        print(f"æµå¼å“åº”å—æ•°: {len(streaming_chunks)}")
        print(f"å®Œæ•´å“åº”æ•°: {len(complete_responses)}")
        print(f"é”™è¯¯æ•°: {len(errors)}")
        
        if complete_responses:
            print("\næ”¶åˆ°çš„å“åº”:")
            for i, response in enumerate(complete_responses, 1):
                preview = response[:100] + "..." if len(response) > 100 else response
                print(f"  {i}. {preview}")
        
        logger.info("âœ… LLMå®¢æˆ·ç«¯æµ‹è¯•å®Œæˆ")
        return len(complete_responses) > 0  # å¦‚æœæ”¶åˆ°å“åº”åˆ™æµ‹è¯•é€šè¿‡
        
    except Exception as e:
        logger.error(f"ğŸ’¥ æµ‹è¯•è¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return False
    
    finally:
        # æ¸…ç†èµ„æº
        if llm_client and hasattr(llm_client, '__aexit__'):
            try:
                await llm_client.__aexit__(None, None, None)
            except:
                pass
        
        # å…³é—­äº‹ä»¶æ€»çº¿
        logger.info("ğŸ§¹ å…³é—­äº‹ä»¶æ€»çº¿...")
        await event_bus.shutdown()

if __name__ == "__main__":
    try:
        # è®¾ç½®äº‹ä»¶å¾ªç¯ç­–ç•¥ï¼ˆWindowså…¼å®¹ï¼‰
        if sys.platform.startswith('win'):
            asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
        
        async def main():
            logger.info("ğŸ¬ å¼€å§‹LLMå®¢æˆ·ç«¯æµ‹è¯•")
            
            # è¿è¡Œæµ‹è¯•
            result = await test_llm_client()
            
            logger.info("=" * 50)
            logger.info(f"ğŸ“Š æµ‹è¯•ç»“æœ: {'ğŸ‰ æµ‹è¯•é€šè¿‡' if result else 'âŒ æµ‹è¯•å¤±è´¥'}")
            return result
        
        # è¿è¡Œæµ‹è¯•
        result = asyncio.run(main())
        
    except KeyboardInterrupt:
        logger.info("â¹ï¸ ç”¨æˆ·ä¸­æ–­æµ‹è¯•")
    except Exception as e:
        logger.error(f"ğŸ’¥ æµ‹è¯•è¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        logger.error(traceback.format_exc())