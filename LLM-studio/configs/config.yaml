# LLM训练配置

# 模型相关配置
model:
  path: "LLM模型路径"  # 模型路径
  use_lora: false  # 默认用lora训练方式。如果想要进行全参数微调，请改为false
  # LoRA特定参数 (仅当use_lora为true时使用)
  lora_target_modules: 
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"
  lora_r: 8
  lora_alpha: 32
  lora_dropout: 0.05

# 训练相关配置
training:
  output_dir: "output"  # 输出目录
  batch_size: 1  # 每设备批次大小
  gradient_accumulation_steps: 16  # 梯度累积步数
  learning_rate: 0.0002 
  warmup_steps: 50
  max_grad_norm: 1.0
  epochs: 3  # 训练轮数
  logging_steps: 10
  save_steps: 10
  save_total_limit: 3  # 保存的检查点数量
  save_on_each_node: true
  gradient_checkpointing: true  # 是否使用梯度检查点
  bf16: true  # 是否使用bfloat16
  fp16: false  # 是否使用float16
  remove_unused_columns: false
  optimizer: "adamw_torch"  # 优化器
  dataloader_pin_memory: true
  group_by_length: true
  lr_scheduler_type: "cosine"  # 学习率调度器类型
  weight_decay: 0.01
  max_steps: -1  # -1表示使用epochs
  save_strategy: "steps"
  save_safetensors: true
  overwrite_output_dir: true

# 数据相关配置
data:
  path: "./train.json"  # 训练数据路径
  max_length: 2048  # 最大序列长度
